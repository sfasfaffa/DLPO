<H1>DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective</H1>

Large Language Models (LLMs) have achieved remarkable success across diverse tasks, largely driven by well-designed prompts. However, crafting and selecting such prompts often requires considerable human effort, significantly limiting its scalability. To mitigate this, recent studies have explored automated prompt optimization as a promising solution. Despite these efforts, existing methods still face critical challenges in \textit{\textbf{robustness, efficiency, and generalization}}.
To systematically address these challenges, we first conduct an empirical analysis to identify the limitations of current reflection-based prompt optimization paradigm.
Building on these insights, we propose 7 innovative approaches inspired by traditional deep learning paradigms for prompt optimization (\texttt{DLPO}), seamlessly integrating these concepts into text-based gradient optimization. 
Through these advancements, we progressively tackle the aforementioned challenges and validate our methods through extensive experimentation.
We hope our study not only provides valuable guidance for future research but also offers a comprehensive understanding of the challenges and potential solutions in prompt optimization.
**Codes are coming soon!**
